
# ðŸš€ Apache Airflow with Multi-Language Spark Job Orchestration with Java, Scala and Python Spark Jobs

This project orchestrates Spark jobs written in **Python**, **Scala**, and **Java** using **Apache Airflow**, all within a **Dockerized environment**. The DAG `sparking_flow` submits Spark jobs in multiple languages to a local Spark cluster, enabling robust, scalable, and repeatable data workflows.

---

## ðŸ”§ Technology Stack & Versions

| Tool        | Version       |
|-------------|---------------|
| Python      | 3.12.x        |
| Java        | 21 (LTS)      |
| Scala       | 3.4.x         |
| Apache Spark| 3.5.x         |
| Apache Airflow | 2.9.x      |
| SBT         | 1.10.x        |
| Docker      | Latest        |
| Docker Compose | Latest    |

---

## ðŸ“ Project Structure

```bash
.
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ spark_airflow.py       # Airflow DAG
â”œâ”€â”€ img/
â”‚   â””â”€â”€ Capture.PNG            # Airflow Graph
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â””â”€â”€ wordcountjob.py    # Python Spark job
â”‚   â”œâ”€â”€ scala/
â”‚   â”‚   â”œâ”€â”€ build.sbt
â”‚   â”‚   â””â”€â”€ wordcountjob.scala # Scala Spark job
â”‚   â””â”€â”€ java/
â”‚       â””â”€â”€ spark-job/         # Java Spark job
â”‚           â””â”€â”€ src/...
â”œâ”€â”€ airflow.env
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â””â”€â”€ .dockerignore
```

### ðŸš€ Apache Airflow Graph with Multi-Language Spark Job Orchestration

![Airflow + Spark Architecture](img/Capture.PNG)


---

## âš™ï¸ Setup Instructions

### ðŸ”¨ Step 1: Initialize Directories and Files

```bash
mkdir dags jobs
touch airflow.env docker-compose.yml Dockerfile .dockerignore
mkdir -p jobs/python jobs/scala jobs/java
touch dags/spark_airflow.py
touch jobs/python/wordcountjob.py
touch jobs/scala/{build.sbt,wordcountjob.scala}
```

> *Optional:* Create the Java job under `jobs/java/spark-job/` following Maven or Gradle structure.

---

### ðŸ³ Step 2: Run the Docker Environment

```bash
docker compose up -d --build
```

To gracefully stop all running services:

```bash
docker compose down
```

To rebuild and restart the environment:

```bash
docker compose up -d --build
```

---

### ðŸŒ Access Web UIs

- **Airflow Web UI:** [http://localhost:8080](http://localhost:8080)  
  > Authenticate using your configured Airflow credentials (configured in the Airflow environment).

- **Spark Master UI:** [http://localhost:9090](http://localhost:9090)

---

## âš¡ Spark Cluster Configuration

A basic Spark setup includes **1 Master** and **1 Worker**.  
To scale up, **duplicate** the Spark Worker section in `docker-compose.yml`, assigning each container a unique name and hostname.

```yaml
spark-worker-2:
  image: bitnami/spark:latest
  container_name: spark-worker-2
  command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  depends_on:
    - spark-master
  environment:
    SPARK_MODE: worker
    SPARK_WORKER_CORES: 2
    SPARK_WORKER_MEMORY: 1g
    SPARK_MASTER_URL: spark://spark-master:7077
```

---

## ðŸ“¦ Install Python & Airflow Dependencies

Inside your development environment or Docker container:

```bash
pip install apache-airflow apache-airflow-providers-apache-spark pyspark
```

---

## âœï¸ DAG Creation

Create the DAG file at `dags/spark_airflow.py`, with tasks that trigger:

- A `PythonOperator` to mark the start
- A `SparkSubmitOperator` for:
  - Python job
  - Scala job
  - Java job
- A `PythonOperator` to mark the end

---

## ðŸ Python Spark Job

`jobs/python/wordcountjob.py`

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("WordCountPython").getOrCreate()
data = spark.read.text("path/to/input.txt")
words = data.selectExpr("explode(split(value, ' ')) as word")
word_counts = words.groupBy("word").count()
word_counts.show()
spark.stop()
```

---

## ðŸ§ª Scala Spark Job

Install Scala and SBT:

```bash
brew install scala sbt
```

`jobs/scala/wordcountjob.scala`

```scala
import org.apache.spark.sql.SparkSession

object WordCountScala {
  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder.appName("WordCountScala").getOrCreate()
    val data = spark.read.textFile("path/to/input.txt")
    val words = data.flatMap(_.split(" "))
    val wordCounts = words.groupByKey(identity).count()
    wordCounts.show()
    spark.stop()
  }
}
```

Compile and package:

```bash
cd jobs/scala
sbt compile
sbt package
```

---

## â˜• Java Spark Job

Follow Maven/Gradle layout. Sample logic should mirror the Scala/Python version.

```bash
mkdir -p jobs/java/spark-job/src/main/java/com/example
# Add Java class and pom.xml/build.gradle accordingly
```

Build the Java JAR:

```bash
cd jobs/java/spark-job
mvn clean package
```

---

## âœ… Airflow DAG Workflow

After successfully deploying the containers:

- Go to Airflow UI: `localhost:8080`
- Activate and manually trigger the DAG: `sparking_flow`

---

## ðŸ”Œ Notes

- You **must** configure the **Spark connection** in Airflow UI:
  - Go to **Admin > Connections > Add Connection**
  - Conn Type: `Spark`
  - Host: `spark://spark-master`
  - Port: `7077`

---

## ðŸŽ¥ Full Tutorial

[![Watch on YouTube](https://img.youtube.com/vi/o_pne3aLW2w/0.jpg)](https://www.youtube.com/watch?v=o_pne3aLW2w)

---

## ðŸ§¼ Cleanup

```bash
docker compose down -v --remove-orphans
```

---

## ðŸ§  Contributions

We welcome contributions via pull requests. Please follow best practices and provide detailed descriptions when contributing.
